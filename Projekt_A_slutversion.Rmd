---
title: "Projekt A - Diamantpris-prediktion"
author: "Piere Ventura Cruz, Axel Johansson & Simon Jorstedt"
date: "`r Sys.Date()`"
output: pdf_document

header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r setup, echo=FALSE}
#Setupkod

#install.packages("fastDummies")
#library(fastDummies)
bool = FALSE
```

# Sammanfattning
I den här rapporten ska vi prediktera diamanters försäljningspris med hjälp av förklarningsvariablerna vikt (carat), färg, klarhet och organisationen som certifierat diamanten. När vi plottar pris mot vikt märker vi att vår regressionslinje passar något bra till datan men att vikten verkar ha ett kvadratiskt beteende. Vi märker också att vi måste logaritmera priset. Vi studerar multikollinearitet med hjälp av VIF och använder oss utav Forward selection samt Backwards elimination för att få fram våra modeller.

# Introduktion
Det primära syftet med denna rapport är att studera, och att försöka prediktera diamanters säljpris. På vägen kommer vi ta en titt på skillnader mellan de certifikerande institutioner som bedömmer diamanterna. Vi kommer sedan gå vidare till det primära syftet genom att studera de tillgängliga förklaringsvariablerna, hur de relaterar till varandra och hur de påverkar priset..

# Data
Datamaterialet består av egenskaperna Carat, färg (\textit{Color purity}), klarhet (\textit{Clarity}), Organisation, och pris (Singaporianska dollar S$) för 308 diamanter. I brist på information om var priset kommer från, antar vi att priset och diamanterna i datamängden är tagna från 308 individuella oberoende försäljningar. Vanligtvis brukar även skurningen \textit{Cut} av en diamant vara intressant, men den variabeln ingår inte datamaterialet.

Carat (ej Karat) är ett viktmått på diamanter, motsvarande $0.2$g. färg är ett mått på en diamants renhet, som anges i de alfabetiska kategorierna D, E, F, G, H... i nedstigande led. För att kunna genomföra statistisk analys har kategorierna översatts till sifferbetygen 6-1. Kategorierna är visserligen D-Z, men endast kategorierna D-H återfinns i datamaterialet. Klarhet anger förekomsten av skrapningar i diamanten. Kategorierna IF, VVS1, VVS2, VS1 och VS2 har översatts till siffervärdena 5-1. Organisation anger den oberoende organisation som utvärderat och utfärdat ett certifikat för en specifik diamant. De tre organisationerna som förekommer i datamängden är Gemological Institute of America (GIA), International Gemological Institute (IGI) samt Hoge Raad Voor Diamant (HRD). De kodas med två dummy-variabler, där GIA betraktas som utgångsläget.

```{r, echo=bool}
data_raw <- read.table("diamant-win.dat")
data <- data_raw
data
#Color
data$V2[data$V2 == 'D'] <- 6
data$V2[data$V2 == 'E'] <- 5
data$V2[data$V2 == 'F'] <- 4
data$V2[data$V2 == 'G'] <- 3
data$V2[data$V2 == 'H'] <- 2
data$V2[data$V2 == 'I'] <- 1

#Clarity
data$V3[data$V3 == 'IF'] <- 5
data$V3[data$V3 == 'VVS1'] <- 4
data$V3[data$V3 == 'VVS2'] <- 3
data$V3[data$V3 == 'VS1'] <- 2
data$V3[data$V3 == 'VS2'] <- 1

#Data formatting (char -> double)
data$V2 = as.numeric(data$V2)
data$V3 = as.numeric(data$V3)

#New column for logarithmic values of V5 (price)
data$V6 = log(data$V5)

#Creating square V1
data$V12 = data$V1^2

data_G = data[1:151,]
#head(data_G)

data_I = data[152:229,]
#head(data_I)

data_H = data[230:308,]
#head(data_H)

zero = rep(0,308)
data$V7 <- zero
data$V8 <- zero
data$V7[data$V4 == 'IGI'] <- 1
data$V8[data$V4 == 'HRD'] <- 1
#head(data)
```

## Institutionerna
Vi plottar upp de tre instutionerna mot varandra för att se hur de skiljer sig när det kommer till vilka sorters diamanter de värderar. Detta visas i figur 1.

```{r, echo=bool, fig.cap="Boxplot för vikt, färg, klarhet och försäljningspris för diamanterna från de tre respektive organisationerna GIA, IGI and HRD.", out.width="80%", fig.align="center"}
par(mfrow=c(2,2))
#layout(matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE))
boxplot(data_G$V1, data_I$V1, data_H$V1,
        names=c("GIA", "IGI", "HRD"),
        main="Carat")

boxplot(data_G$V2, data_I$V2, data_H$V2,
        names=c("GIA", "IGI", "HRD"),
        main="Color")

boxplot(data_G$V3, data_I$V3, data_H$V3,
        names=c("GIA", "IGI", "HRD"),
        main="Clarity")

boxplot(data_G$V5, data_I$V5, data_H$V5,
        names=c("GIA", "IGI", "HRD"),
        main="Försäljningspris (S$)")

```

Tydligen skiljer sig diamanterna från de tre certifikatutfärdarna mycket i Carat, Clarity och slutpris. Anledningen till dessa tydliga skillnader är troligtvis antingen att de olika institutionerna tilldelas olika typer av diamanter att certifiera, eller det något \textit{otroligare} alternativet att institutionerna och därför skulle bedömma en hypotetisk diamant med olika "betyg".

# Förklaringsvariabler
Vi skall inleda med att ta en titt på de aktuella förklaringsvariablerna, hur de relaterar till priset, och hur de relaterar till varandra.

## Vikt
Vi inleder med att plotta priset mot vikten i figur 2. Vikten är den enda icke-kategoriska variabeln och därför en naturlig utgångspunkt. Vi konstruerar även en enkel reglinje för pris mot vikt i figur 2.

```{r, echo=bool, fig.cap="Pris plottad mot vikt, samt enkel regressionslinje.", out.width="80%", fig.align="center"}
mod1 = lm(data$V5 ~ data$V1)
mod1
plot(data$V1, data$V5,
     xlab="Vikt (Carat)",
     ylab="Pris (S$)")
abline(mod1$coefficients[1], mod1$coefficients[2])
```

I figur 2 ser vi en något sånär bra anpassning, men det är tydligt att något är fel. Priset verkar uppvisa ett icke-linjärt beteende, möjligen kvadratiskt. För att undersöka detta ytterligare plottar vi anpassningens residualer i figur 3.

```{r, echo=bool, fig.cap="Residualer för den enkla linjära anpassningen i figur 2: lm(pris = vikt).", out.width="50%", fig.align="center"}

mod1_5 = lm(V5 ~ V1, data)
plot(mod1_5, which=c(1))
abline(a=0,b=0, col="grey")
```

I figur 3 är det möjligen kvadratiska beteendet ännu tydligare. För att bättre beskriva sambandet mellan vikt och pris lägger vi därför till en term i den enkla modellen motsvarande vikten i kvadrat. I figur 4 ser vi residualerna för den modellen.

```{r, echo=bool, fig.cap="Residualer för en anpassad modell med vikt, samt vikt i kvadrat som förklaringsvariabler för priset. Modellen: lm(pris = vikt + vikt$^2$).", out.width="50%", fig.align="center"}

mod1kvad_5 = lm(V5 ~ V1 + I(V1^2), data)
plot(mod1kvad_5, which=c(1))
abline(a=0,b=0, col="grey")

#summary(mod1kvad_5)
```

I figur 4 är residualerna utjämnade längs 0-linjen, men det råder fortfarande heteroskedecitet. För större vikter ökar alltså vårt anpassningsfel, medan felet är lågt för små vikter. Vi försöker åtgärda detta genom att logaritmera responsvariabeln pris. Vi anpassar alltså en model med vikt, samt kvadrerad vikt som förklaringsvariabler, och det logaritmerade priset som responsvariabel. Vi använder den naturliga logaritmen. Resultatet av detta  framställs i figur 5.

```{r, echo=bool, fig.cap="Residualer för den anpassade modellen med förklaringsvariablerna vikt och kvadrerad vikt, och logaritmerat pris som responsvariabel. Modellen: lm(log[pris] = vikt + vikt$^2$).", out.width="50%", fig.align="center"}

mod1kvad_6 = lm(V6 ~ V1 + I(V1^2), data)
plot(mod1kvad_6, which=c(1))
abline(a=0,b=0, col="grey")
```

Residualplotten i figur 5 är mycket jämnare än i figur 4. Tydligen har vi lyckats åtgärda heteroskedeciteten. Vi skall även i fortsättningen betrakta det logaritmerade priset som vår responsvariabel.

## Färg och Klarhet
Vi skall nu gå vidare med att studera färg och klarhet. Vi fortsätter med att betrakta det logaritmerade priset som responsvariabel för att kunna föra samman förklaringsvariablerna i samma modell senare. I figur Ä ser vi det logaritmerade priset plottat mot färg respektive klarhet, samt motsvarande enkla regressionslinjer.

```{r, echo=bool, out.width="80%", fig.align="center", fig.cap="Logaritmerat pris plottat mot färg respektive klarhet, samt enkla reglinjer."}
layout(matrix(c(1,2), 1, 2, byrow = TRUE))

mod2 = lm(data$V6 ~ data$V2)
plot(data$V2, data$V6, col="green",
     xlab="Färg",
     ylab="Logaritmerat pris")
abline(mod2$coefficients[1], mod2$coefficients[2])

mod3 = lm(data$V6 ~ data$V3)
plot(data$V3, data$V6, col="blue",
     xlab="Klarhet",
     ylab="Logaritmerat pris")
abline(mod3$coefficients[1], mod3$coefficients[2])
```

Det vi ser i figur 6 är att det tydligen råder ett negativt samband mellan klarhet och logaritmerat pris. Men det är missvisande. Som tidigare konstaterats är institutionen IGI överrepresenterad bland diamanter med låg vikt, såväl som hög clarity. Eftersom vikt verkar ha en mer betydande effekt på priset än klarhet så "maskeras" därför effekten av klarheten och framställs som negativ. I själva verket är klarheten \textit{troligtvis} associerat med högre pris (och därför även logaritmerat pris) i en diamant-population som är oberoende från institutionerna.

I figur 6 ser vi även att det verkar råda en viss svag (positiv) korrelation mellan färg och logaritmerat pris. Det resultatet har vi mer tilltro till än clarity-beteendet, eftersom vi i figur 1 sett att color verkar vara jämt fördelat mellan institutionerna.

## Institutionerna
De tre institutionerna har kodats med två dummy-variabler för att kunna inkluderas i analys och modellkonstruktion. Vi väljer GIA till grund-läge, eftersom diamanterna därifrån verkar någorlunda jämt fördelade över egenskaperna vikt, pris, färg och klarhet.

## Multikollinearitet
Vi skall nu studera eventuell multikollinearitet i förklaringsvariablerna vikt, kvadrerad vikt, färg och klarhet. Vi gör detta genom att beräkna de respektive VIF-värdena. Det är oklart hur ett VIF-värde för Dummyvariablerna skulle beräknas (på ett meningsfullt sätt), så det utelämnar vi. Men däremot tas dummyvariablerna med i beräkningen av de övriga VIF-värdena.

```{r, echo=bool}
mod1 = lm(V1 ~ V2 + V3 + I(V1^2) + V7 + V8, data)
mod2 = lm(V2 ~ V3 + I(V1^2) + V7 + V8 + V1, data)
mod3 = lm(V3 ~ I(V1^2) + V7 + V8 + V1 + V2, data)
mod1kvad = lm(I(V1^2) ~ V7 + V8 + V1 + V2 + V3, data)

VIF1 = 1/(1-summary(mod1)$adj.r.squared)
VIF2 = 1/(1-summary(mod2)$adj.r.squared)
VIF3 = 1/(1-summary(mod3)$adj.r.squared)
VIF4 = 1/(1-summary(mod1kvad)$adj.r.squared)

#VIF1
#VIF2
#VIF3
#VIF4
```

Gör vi detta så får vi resultaten $VIF_{vikt} \approx$ `r round(VIF1,2)`, $VIF_{färg} \approx$ `r round(VIF2,2)`, $VIF_{klarhet} \approx$ `r round(VIF3,2)`, samt $VIF_{vikt^2} \approx$ `r round(VIF4,2)`. Det framgår att vikt och vikt i kvadrat är multikollineära, vilket är fullt rimligt. Färg och klarhet är inte signifikanta. Vi konstruerar tre modeller med olika kombinationer av vikt och kvadrerad vikt som förklaringsvariabler, och det logaritmerade priset som responsvariabel och jämför dem. Följande är summary av dessa tre anpassade modeller.

```{r, echo=bool}
m1 = lm(V6 ~ V1, data)
summary(m1)

m2 = lm(V6 ~ I(V1^2), data)
summary(m2)

m3 = lm(V6 ~ V1 + I(V1^2), data)
summary(m3)
```

Vi kan återigen se att vikten ensam ger en mycket hög (justerad) förklaringsgrad $R_{v}^2 \approx 0.94$, medan kvadrerad vikt ger en något lägre: $R_{v^2}^2 \approx 0.84$. Tillsammans blir modellen bara snäppet bättre ($R_{v+v^2}^2 \approx 0.96$) än när vi endast inkluderade vikten. Detta skulle kunna leda oss till att avfärda den kvadrerade vikten, men vi har tidigare konstaterat förekomsten av det kvadratiska sambandet och väljer därför att ändå behålla den kvadrerade vikten.

# Modellval och prediktion
Vi skall nu studera och försöka konstruera en bra modell som beskriver datan tillfredställande, och som förhoppningsvis även har bra prediktiva förmågor.

## Forward selection
Vi börjar med att genomföra forward selection-metoden, som vi anpassat för situationen enligt följande: Vi börjar med en konstant modell, och lägger till den förklaringsvariabel som ger det högsta (justerade) R^2-värdet, om den förklaringsvariabels parameter i den modellen är signifikant på 95%-nivån. Vi fortsätter på detta sätt, tills vi uppnått ett (justerat) R^2-värde på minst $0.99$.

Anledningarna till denna utformning och att vi valt att jämföra R^2-värdena är bland annat att två p-värden erhålls som modelljämförelsemått när V7 och V8 läggs till, vilket är svårtolkat. Genom att ändå lägga till kravet att tillagda förklaringsvariabler ska ha signifikant parameter tar vi hänsyn till eventuellt osäkra parametrar.

När denna metod genomförs inkluderas vikt, sedan kvadrerad vikt, färg, och slutligen klarhet i en modell med ett (justerat) R^2-värde på ca $0.995$. Denna modell kallar vi \textit{VV2FK}.


```{r, echo=bool}
##FORWARD SELECTION
#EN VARIABEL
mod1 = lm(V6 ~ V1, data)
mod2 = lm(V6 ~ V2, data)
mod3 = lm(V6 ~ V3, data)
mod78 = lm(V6 ~ V7 + V8, data)
mod1kvad = lm(V6 ~ I(V1^2), data)

#summary(mod1)$adj.r.squared
#summary(mod2)$adj.r.squared
#summary(mod3)$adj.r.squared
#ummary(mod78)$adj.r.squared
#summary(mod1kvad)$adj.r.squared

#V1 är mest signifikant.
#summary(mod1)
#1111111111111 #Separerare
#TVÅ VARIABLER
mod12 = lm(V6 ~ V1 + V2, data)
mod13 = lm(V6 ~ V1 + V3, data)
mod178 = lm(V6 ~ V1 + V7 + V8, data)
mod11kvad = lm(V6 ~ V1 + I(V1^2), data)

#summary(mod12)$adj.r.squared
#summary(mod13)$adj.r.squared
#summary(mod178)$adj.r.squared
#summary(mod11kvad)$adj.r.squared

#V1^2 ger mest signifikant R^2-adj.
#summary(mod11kvad)
#1111111111111 #Separerare
#TRE VARIABLER
mod11kvad2 = lm(V6 ~ V1 + I(V1^2) + V2, data)
mod11kvad3 = lm(V6 ~ V1 + I(V1^2) + V3, data)
mod11kvad78 = lm(V6 ~ V1 + I(V1^2) + V7 + V8, data)

#summary(mod11kvad2)$adj.r.squared
#summary(mod11kvad3)$adj.r.squared
#summary(mod11kvad78)$adj.r.squared

#V2 ger ett något bättre resultat
#summary(mod11kvad2)
#1111111111111 #Separerare
#FYRA VARIABLER
mod11kvad23 = lm(V6 ~ V1 + I(V1^2) + V2 + V3, data)
mod11kvad278 = lm(V6 ~ V1 + I(V1^2) + V2 + V7 + V8, data)

#summary(mod11kvad23)$adj.r.squared
#summary(mod11kvad278)$adj.r.squared

#V3 ger ett aningen bättre resultat
#summary(mod11kvad23)
#1111111111111 #Separerare
#FEM VARIABLER
mod11kvad2378 = lm(V6 ~ V1 + I(V1^2) + V2 + V3 + V7 + V8, data)

#summary(mod11kvad2378)$adj.r.squared
```


## Backwards elimination
Vår backwards elimination-metod specifierar vi enligt följande: Först inkluderas alla förklaringsvariabler. Det vill säga vikt, kvadrerad vikt, färg, klarhet samt Dummyvariablerna. Vi utesluter den förklaringsvariabel som motsvarar minst minskning i (justerade) R^2-värdet när den tas bort. Vi fortsätter utesluta förklaringsvariabler på det här sättet, så länge den kvarvarande modellen minst har (justerat) R^2-värde $0.95$. Vi kräver även att samtliga parametrar i en kvarvarande modell skall vara signifikanta på 95%-nivån.

Med denna specifikation har den inledande modellen ett justerat R^2-värde på ca $0.995$. Vi utesluter först Dummyvariablerna, sedan färg, och slutligen klarhet. Vi kan inte dessutom utesluta antingen vikt, eller kvadrerad vikt utan att modellens (justerade) R^2-värde understiger $0.95$. Denna modell kallar vi \textit{VV2}.

```{r, echo=bool}
##BACKWARDS ELIMINATION
#RUNDA 1
modall = lm(V6 ~ V1 + V2 + V3 + V7 + V8 + I(V1^2), data)
modall_1 = lm(V6 ~ V2 + V3 + V7 + V8 + I(V1^2), data)
modall_2 = lm(V6 ~ V1 + V3 + V7 + V8 + I(V1^2), data)
modall_3 = lm(V6 ~ V1 + V2 + V7 + V8 + I(V1^2), data)
modall_78 = lm(V6 ~ V1 + V2 + V3 + I(V1^2), data)
modall_1kvad = lm(V6 ~ V1 + V2 + V3 + V7 + V8, data)

#summary(modall)$adj.r.squared
#summary(modall_1)$adj.r.squared
#summary(modall_2)$adj.r.squared
#summary(modall_3)$adj.r.squared
#summary(modall_78)$adj.r.squared
#summary(modall_1kvad)$adj.r.squared

#summary(modall_78)
#Uteslut V78

#RUNDA 2
modall_781 = lm(V6 ~ V2 + V3 + I(V1^2), data)
modall_782 = lm(V6 ~ V1 + V3 + I(V1^2), data)
modall_783 = lm(V6 ~ V1 + V2 + I(V1^2), data)
modall_781kvad = lm(V6 ~ V1 + V2 + V3, data)

#summary(modall_781)$adj.r.squared
#summary(modall_782)$adj.r.squared
#summary(modall_783)$adj.r.squared
#summary(modall_781kvad)$adj.r.squared

#summary(modall_783)
#uteslut V3

#RUNDA 3
modall_7831 = lm(V6 ~ V2 + I(V1^2), data)
modall_7832 = lm(V6 ~ V1 + I(V1^2), data)
modall_7831kvad = lm(V6 ~ V1 + V2, data)

#summary(modall_7831)$adj.r.squared
#summary(modall_7832)$adj.r.squared
#summary(modall_7831kvad)$adj.r.squared

#summary(modall_7832)
#Uteslut V2

#RUNDA 4
modall_78321 = lm(V6 ~ I(V1^2), data)
modall_78321kvad = lm(V6 ~ V1, data)

#summary(modall_78321)$adj.r.squared
#summary(modall_78321kvad)$adj.r.squared

#Uteslut varken V1 eller V1^2.
```

# Utvärdering
Innan vi utvärderar modellerna \textit{VV2FK} och \textit{VV2}, så behöver vi återvända till responsvariabeln. Alla våra anpassningar har varit av det logaritmerade priset, varför vi nu behöver lösa ut priset $P$ genom att höja båda leden med $e$. Vi betecknar vikt med $V$, kvadrerad vikt med $V^2$, färg med $F$, samt klarhet med $K$. Modell \textit{VV2FK} blir då

$$P_i = exp(\alpha + \beta_1 V_i + \beta_2 V_i^2 + \beta_3 F_i + \beta_4 K_i) = e^\alpha \cdot e^{\beta_1 V_i} \cdot e^{\beta_2 V_i^2} \cdot e^{\beta_3 F_i} \cdot e^{\beta_4 K_i}$$

På liknande sätt får vi att modell \textit{VV2} blir

$$P_i = exp(\alpha + \beta_1 V_i + \beta_2 V_i^2) = e^\alpha \cdot e^{\beta_1 V_i} \cdot e^{\beta_2 V_i^2}$$

Notera att parametrarna i de två modellerna är olika, men notationen återanvänds för att undvika onödigt komplex notation. Exempelvis gäller att $\beta_1$ från \textit{VV2FK} inte är lika med $\beta_1$ från modell \text{VV2}.

## MSEP
Nu när modellerna är konstruerade vill vi utvärdera deras prediktionsförmågor. För enkelhetens skull beräknar vi MSEP-värdena för modellversionerna med det logaritmerade priset som respons. Det förändrar inte resultatet, eftersom vi ju bara är intresserade av vilken modell som har det största MSEP-värdet.

```{r, echo=bool}
#Hårdkodad MSEP (main MSEP)
#VVFK

sq_sum = 0
for (i in 1:308)
{
        #Anpassa modell
        model = lm(V6 ~ V1 + V12 + V2 + V3, data[-i,])
        
        #Ny dataframe för anpassning
        new_frame = data.frame(V1=data$V1, V2=data$V2, V3=data$V3, V12=data$V12)
        new_frame$y_pred = predict.lm(model, newdata=new_frame)
        
        
        #Create square terms
        #print(c(data[[6]][i], new_frame$y_pred[i]))
        add = (data[[6]][i] - new_frame$y_pred[i])^2
        
        sq_sum = sq_sum + add
}
sq_sum = sq_sum/308

VVFK_msep = sq_sum
```


```{r, echo=bool}
#Hårdkodad MSEP
#VV2

sq_sum = 0
for (i in 1:308)
{
        #Anpassa modell
        model = lm(V6 ~ V1 + V12, data[-i,])
        
        #Ny dataframe för anpassning
        new_frame = data.frame(V1=data$V1, V12=data$V12)
        new_frame$y_pred = predict.lm(model, newdata=new_frame)
        
        
        #Create square terms
        #print(c(data[[6]][i], new_frame$y_pred[i]))
        add = (data[[6]][i] - new_frame$y_pred[i])^2
        
        sq_sum = sq_sum + add
}
sq_sum = sq_sum/308

VV2_msep = sq_sum
```

Vi får då MSEP-värdena `r round(VVFK_msep, 10)` (\textit{VV2FK}) och `r round(VV2_msep, 10)` (\textit{VV2}). Det lägre värdet för \textit{VV2FK} säger oss att \textit{VV2FK} är bättre på att prediktera ny data (av ungefärlig samma typ som i datamängden) än \textit{VV2}. Men denna skillnad är mycket liten, och beroende på syftet är det mycket möjligt att den är försumbar.

# Resultat
Den bästa modellen som vi kommit fram till är slutligen \textit{VV2FK} som kan avrundas till följande:

```{r, echo=bool}
#summary(mod11kvad23)
#summary(modall_7832)


#mod11kvad23$coefficients
#modall_7832$coefficients
```


$$P_i = e^{5.14} \cdot e^{5.72 V_i} \cdot e^{-2.14 V_i^2} \cdot e^{0.09 F_i} \cdot e^{0.08 K_i}$$
Det skall återigen poängteras att även modell \textit{VV2} var en god anpassning till data, och den kan avrundas till följande:

$$P_i = e^{5.78} \cdot e^{5.44 V_i} \cdot e^{-2.05 V_i^2}$$

\textit{VV2} har dessutom fördelen att kunna plottas meningsfullt i två dimensioner. Detta ses i figur 7.

```{r, echo=bool, fig.cap="Pris plottad mot vikt, tillsammans med kurva för modell VV2.", fig.align="center", out.width="80%"}
#mod1kvad_6 beräknas ovan

a=mod1kvad_6$coefficients[1]
b=mod1kvad_6$coefficients[2]
c=mod1kvad_6$coefficients[3]

plot(data$V1, data$V5,
     xlab="Vikt (Carat)",
     ylab="Pris (S$)")
curve(exp(a)*exp(b*x)*exp(c*x^2), add=TRUE)
```


